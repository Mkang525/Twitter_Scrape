{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import boto3\n",
    "import os\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run lib/__wesley_init_verbose__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['AWS_PROFILE'] = \"profile\"\n",
    "# os.environ['AWS_DEFAULT_REGION'] = \"us-east-1\"\n",
    "\n",
    "# create client object\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all s3 buckets in json format\n",
    "response = s3.list_buckets()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check S3 bucket\n",
    "# print s3 bucket to make sure linked\n",
    "\n",
    "def status_df(response):\n",
    "    return pd.DataFrame({\n",
    "    'ts': [(x['CreationDate']) for x in response['Buckets']],\n",
    "    'bucket name': [x['Name'] for x in response['Buckets']]  \n",
    "})\n",
    "\n",
    "status_df(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create resource object\n",
    "s3_resource = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access file from s3 include bucket name and key\n",
    "obj = s3_resource.Object(bucket_name='sandy-hook-data', key='sandy_one_year copy.csv')\n",
    "\n",
    "# use stringIO to read object\n",
    "s3_data = StringIO(obj.get()['Body'].read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull data into pandast dataframe and read csv\n",
    "data = pd.read_csv(s3_data)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ben's code\n",
    "# Import the books.csv file as a DataFrame\n",
    "# why show vertical instead of horizontal?\n",
    "df = pd.read_csv('../sandy_one_year copy.csv')\n",
    "df.sample(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('From the \"../sandy_one_year copy.csv\" file.\\n')\n",
    "print('CSV File Shape:\\n',df.shape)\n",
    "print('\\n\\nCSV Column Titles:\\n',df.columns)\n",
    "# print('\\n\\nHashtag Value Counts:\\n', df.hashtags.value_counts())\n",
    "\n",
    "#--------------------------------------------\n",
    "# Make hashtag column into list of values\n",
    "\n",
    "all_tags = []\n",
    "for b in df['hashtags']:\n",
    "    c = str(b).split(\", \")\n",
    "    c = str(c).replace('[', '')\n",
    "    c = str(c).replace(']', '')\n",
    "    c = str(c).replace(\"'\", '')\n",
    "    c = str(c).replace('\"', '')\n",
    "    c = str(c).split(', ')\n",
    "    for element in c:\n",
    "        all_tags.append(element)\n",
    "            \n",
    "# ----------------------------------------------\n",
    "# Find the top 10 hashtags\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "hashtag_count = dict(Counter(all_tags))\n",
    "\n",
    "print('\\n\\nTop 10 Most Frequent Hashtags: \\n', sorted(hashtag_count, key=hashtag_count.get, reverse=True)[:10])\n",
    "#  or \n",
    "print('\\n\\nCount of Top 10 Most Frequent Hashtags: \\n',dict(Counter(hashtag_count).most_common(10)))\n",
    "\n",
    "# --- username frequency ------------------------\n",
    "\n",
    "usernames = []\n",
    "for b in df['username']:\n",
    "    usernames.append(b)\n",
    "\n",
    "username_count = dict(Counter(usernames))\n",
    "\n",
    "print('\\n\\nTop 10 Most Frequent Usernames: \\n', sorted(username_count, key=username_count.get, reverse=True)[:10])\n",
    "#  or \n",
    "print('\\n\\nCount of Top 10 Most Frequent Usernames: \\n',dict(Counter(username_count).most_common(10)), '\\n\\n',)\n",
    "\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'EDT', 'Eastern Standard Time' and 'Eastern Daylight Time'\n",
    "\n",
    "'''preparing a csv to use for further analysis'''\n",
    "\n",
    "# ---Import libraries---\n",
    "%run lib/__wesley_init__.py\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import *\n",
    "from tqdm import tqdm\n",
    "from decimal import Decimal\n",
    "\n",
    "print('Libraries loaded') #Progress check \n",
    "\n",
    "\n",
    "# ---Functions---\n",
    "\n",
    "def hashtags_used_count(username):\n",
    "    from collections import Counter\n",
    "    username_sorted_df = df[df['username'] == username]\n",
    "    all_tags = []\n",
    "    for b in username_sorted_df['hashtags']:\n",
    "        c = str(b).split(\", \")\n",
    "        c = str(c).replace('[', '')\n",
    "        c = str(c).replace(']', '')\n",
    "        c = str(c).replace(\"'\", '')\n",
    "        c = str(c).replace('\"', '')\n",
    "        c = str(c).split(', ')\n",
    "        for element in c:\n",
    "            all_tags.append(element)\n",
    "    \n",
    "    hashtag_count = dict(Counter(all_tags))\n",
    "    return hashtag_count\n",
    "\n",
    "def min_max_date(username):\n",
    "    from collections import Counter\n",
    "    username_sorted_df = df[df['username'] == username]\n",
    "    username_sorted_df = username_sorted_df.reset_index(drop=True)\n",
    "    min_date = min(username_sorted_df['date'])\n",
    "    max_date = max(username_sorted_df['date'])\n",
    "    return min_date, max_date\n",
    "\n",
    "def count_days(min_date, max_date):\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "    min_date = datetime.strptime(min_date, date_format)\n",
    "    max_date = datetime.strptime(max_date, date_format)\n",
    "    delta = max_date - min_date\n",
    "    return delta.days\n",
    "\n",
    "def extract_mentions(df):\n",
    "    mentions = []\n",
    "    for tweet in df['tweet']:\n",
    "        tokened_tweet = tokenize_by_words2(tweet)\n",
    "        for word in tokened_tweet:\n",
    "            if '@' in word:\n",
    "                mentions.append(word)\n",
    "    mention_count = Counter(mentions)\n",
    "    return mention_count\n",
    "\n",
    "# change to eastern/standard/daylight savings\n",
    "#  df.loc[iterator, 'created_at'] = df.loc[iterator, 'created_at'].replace(' PDT', '')\n",
    "#             df.loc[iterator, 'created_at'] = df.loc[iterator, 'created_at'].replace(' Pacific Standard Time', '')\n",
    "#             df.loc[iterator, 'created_at'] = df.loc[iterator, 'created_at'].replace(' Pacific Daylight Time', '')\n",
    "def prep_created_at_column(df, csv_name):\n",
    "    iterator = 0\n",
    "    counter = 0\n",
    "    fmt = '%Y-%m-%d %H:%M:%S'\n",
    "    for _ in tqdm(df.created_at):\n",
    "        if counter < 10000: #This is an autosave feature for really large files\n",
    "            df.loc[iterator, 'created_at'] = df.loc[iterator, 'created_at'].replace(' EST', '')\n",
    "            df.loc[iterator, 'created_at'] = df.loc[iterator, 'created_at'].replace(' EDT', '')\n",
    "            df.loc[iterator, 'created_at'] = df.loc[iterator, 'created_at'].replace(' Eastern Standard Time', '')\n",
    "            df.loc[iterator, 'created_at'] = df.loc[iterator, 'created_at'].replace(' Eastern Daylight Time', '')\n",
    "            iterator += 1\n",
    "            counter += 1\n",
    "        else:\n",
    "            df.to_csv(csv_name+'.csv')\n",
    "            counter = 0\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'],infer_datetime_format=True )\n",
    "\n",
    "def avg_minutes_between_tweets(df):\n",
    "    num_list = []\n",
    "    iterator = 0\n",
    "    for _ in range(len(df)-1):\n",
    "        diff = (df.loc[iterator, 'created_at']) - (df.loc[iterator+1, 'created_at'])\n",
    "        diff_minutes = (diff.days * 24 * 60) + (diff.seconds/60)\n",
    "        if diff_minutes < 0:\n",
    "            diff_minutes = 0\n",
    "        num_list.append(diff_minutes)\n",
    "        iterator += 1\n",
    "    try: avg = sum(num_list) / len(num_list)\n",
    "    except ZeroDivisionError: avg = 0\n",
    "    return float(avg), min(num_list), max(num_list)\n",
    "\n",
    "print('Functions loaded') #Progress Check\n",
    "\n",
    "# ---Establish dataframes---\n",
    "\n",
    "file = input('Which file name would you like me to read (minus the suffix)? ')\n",
    "\n",
    "df = pd.read_csv(file+'.csv')\n",
    "print('Main DataFrame loaded') #Progress Check\n",
    "print('Changing \"created_at\" column to datetime')\n",
    "prep_created_at_column(df, file+'2')\n",
    "print('Created_at column changed') #Progress Check\n",
    "df.to_csv(file+'2.csv')\n",
    "refined_df = pd.DataFrame()\n",
    "\n",
    "print('DataFrames Loaded') #Progress Check\n",
    "\n",
    "\n",
    "#---Processing script---\n",
    "\n",
    "# --- username frequency ---\n",
    "\n",
    "usernames = []\n",
    "for b in df['username']:\n",
    "    usernames.append(b)\n",
    "\n",
    "username_count = dict(Counter(usernames))\n",
    "\n",
    "print('username count completed')\n",
    "\n",
    "# --- processing ---\n",
    "iterator = 0\n",
    "for handle in tqdm(sorted(username_count, key=username_count.get, reverse=True)):\n",
    "    print('Processing:', handle) #Progress Check\n",
    "    temp_df = df[df['username'] == handle] #Establishes a temp sorted df of each username encountered\n",
    "    print(handle, 'DataFrame established') #Progress Check\n",
    "    temp_df = temp_df.reset_index(drop=True)\n",
    "    refined_df.loc[iterator, 'username'] = handle\n",
    "    refined_df.loc[iterator, 'total_tweets'] = int(len(temp_df))\n",
    "    refined_df.loc[iterator, 'total_retweets'] = [dict(Counter(list(temp_df['retweet'])))]\n",
    "    refined_df.loc[iterator, 'hashtag_frequency'] = [dict(Counter(hashtags_used_count(handle)).most_common())]\n",
    "    try: min_date, max_date = min_max_date(handle)\n",
    "    except ValueError: continue\n",
    "    refined_df.loc[iterator, 'min_date'] = (min_date)\n",
    "    refined_df.loc[iterator, 'max_date'] = (max_date)\n",
    "    refined_df.loc[iterator, 'date_range_day_count'] = count_days(min_date, max_date)\n",
    "    mentions = [extract_mentions(temp_df)]\n",
    "    refined_df.loc[iterator, 'mentions'] = mentions\n",
    "    try: avg, min_between, max_between = avg_minutes_between_tweets(temp_df)\n",
    "    except ValueError: continue\n",
    "    refined_df.loc[iterator, 'avg_min_between_tweets'] = float(round(Decimal(str(avg)), 2))\n",
    "    refined_df.loc[iterator, 'min_min_between_tweets'] = float(round(Decimal(str(min_between)), 2))\n",
    "    refined_df.loc[iterator, 'max_min_between_tweets'] = float(round(Decimal(str(max_between)), 2))\n",
    "    \n",
    "    iterator += 1\n",
    "    \n",
    "refined_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
